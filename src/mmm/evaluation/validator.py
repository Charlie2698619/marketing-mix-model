"""
Model Evaluation and Validation Module

This module provides comprehensive model evaluation capabilities for MMM,
including temporal validation, digital checks, feature validation, and
performance metrics according to configuration standards.
"""

import logging
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union
import pickle
import warnings
from datetime import datetime, timedelta
import json
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score

from ..config import load_config
from ..utils.logging import setup_logging


def _safe_config_get(config_obj, key, default=None):
    """Safely get config value from either Pydantic object or dictionary."""
    if hasattr(config_obj, key):
        return getattr(config_obj, key)
    elif isinstance(config_obj, dict) and key in config_obj:
        return config_obj[key]
    else:
        return default


def evaluate_model(
    model_results: Any,
    features_df: pd.DataFrame,
    config: Optional[Any] = None,
    run_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Comprehensive model evaluation with temporal validation and digital checks.
    
    Args:
        model_results: Trained MMM model results
        features_df: Feature-engineered dataframe with actuals
        config: Configuration object (if None, loads from main.yaml)
        run_id: Optional run identifier for tracking
        
    Returns:
        Dict containing all evaluation results and metrics
    """
    logger = logging.getLogger(__name__)
    
    if config is None:
        config = load_config()
    
    eval_config = config.evaluation
    logger.info("Starting comprehensive model evaluation...")
    
    # Initialize evaluation results
    evaluation_results = {
        'run_id': run_id or f"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'timestamp': datetime.now().isoformat(),
        'model_summary': {},
        'validation_results': {},
        'digital_checks': {},
        'feature_validation': {},
        'metrics': {},
        'recommendations': []
    }
    
    try:
        # Extract target variable and predictions
        target_col = config.data.revenue_col
        if target_col not in features_df.columns:
            # Try common alternative names
            possible_targets = ['revenue', 'sales', 'conversions', target_col.lower()]
            target_col = next((col for col in features_df.columns if any(pt in col.lower() for pt in possible_targets)), None)
            
        if target_col is None:
            logger.error("Target variable not found in features dataframe")
            return evaluation_results
        
        actuals = features_df[target_col].values
        
        # Get model predictions (simplified - would use actual model predictions)
        predictions = _generate_model_predictions(model_results, features_df, config)
        
        # 1. Core Performance Metrics
        logger.info("Step 1: Calculating core performance metrics...")
        evaluation_results['metrics'] = _calculate_core_metrics(
            actuals, predictions, eval_config, logger
        )
        
        # 2. Temporal Validation
        logger.info("Step 2: Running temporal validation...")
        if _safe_config_get(eval_config.validation_strategies, 'temporal_holdout', True):
            evaluation_results['validation_results']['temporal'] = _temporal_holdout_validation(
                features_df, target_col, model_results, config, logger
            )
        
        # 3. Digital Channel Checks
        logger.info("Step 3: Performing digital channel checks...")
        evaluation_results['digital_checks'] = _digital_channel_validation(
            features_df, model_results, eval_config, logger
        )
        
        # 4. Advanced Feature Validation
        logger.info("Step 4: Validating advanced features...")
        evaluation_results['feature_validation'] = _advanced_feature_validation(
            features_df, model_results, eval_config, logger
        )
        
        # 5. Model Summary and Diagnostics
        logger.info("Step 5: Generating model diagnostics...")
        evaluation_results['model_summary'] = _generate_model_summary(
            model_results, features_df, actuals, predictions, logger
        )
        
        # 6. Generate Recommendations
        logger.info("Step 6: Generating recommendations...")
        evaluation_results['recommendations'] = _generate_recommendations(
            evaluation_results, eval_config, logger
        )
        
        # Calculate overall validation score
        evaluation_results['overall_score'] = _calculate_overall_score(evaluation_results)
        
        logger.info("âœ… Model evaluation completed successfully")
        logger.info(f"Overall validation score: {evaluation_results['overall_score']:.3f}")
        
        return evaluation_results
        
    except Exception as e:
        logger.error(f"Model evaluation failed: {str(e)}")
        evaluation_results['error'] = str(e)
        return evaluation_results


def _generate_model_predictions(
    model_results: Any, 
    features_df: pd.DataFrame, 
    config: Any
) -> np.ndarray:
    """Generate model predictions (simplified implementation)."""
    # This would use actual model prediction logic
    # For now, simulate predictions based on features
    
    # Get spend columns
    spend_columns = [col for col in features_df.columns if '_SPEND' in col.upper()]
    
    if not spend_columns:
        # Fallback to simple linear relationship
        return features_df.iloc[:, 0].values * 0.8 + np.random.normal(0, 0.1, len(features_df))
    
    # Simple linear combination of spend variables
    spend_data = features_df[spend_columns].fillna(0)
    
    # Use ROI priors from config if available
    roi_bounds = getattr(getattr(config, 'model', {}), 'priors', {}).get('roi_bounds', {})
    
    predictions = np.zeros(len(features_df))
    for col in spend_columns:
        channel_name = col.replace('_SPEND', '').lower()
        if channel_name in roi_bounds:
            roi = np.mean(roi_bounds[channel_name])
        else:
            roi = 2.0  # Default ROI
        
        predictions += spend_data[col].values * roi
    
    # Add some realistic noise
    noise_std = np.std(predictions) * 0.1
    predictions += np.random.normal(0, noise_std, len(predictions))
    
    return predictions


def _calculate_core_metrics(
    actuals: np.ndarray,
    predictions: np.ndarray, 
    eval_config: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Calculate core performance metrics."""
    
    # Handle NaN values
    valid_mask = ~(np.isnan(actuals) | np.isnan(predictions))
    actuals_clean = actuals[valid_mask]
    predictions_clean = predictions[valid_mask]
    
    if len(actuals_clean) == 0:
        logger.warning("No valid predictions for metric calculation")
        return {'error': 'No valid predictions'}
    
    # Calculate metrics
    mape = mean_absolute_percentage_error(actuals_clean, predictions_clean)
    rmse = np.sqrt(mean_squared_error(actuals_clean, predictions_clean))
    r2 = r2_score(actuals_clean, predictions_clean)
    
    # SMAPE (Symmetric MAPE)
    smape = np.mean(2 * np.abs(predictions_clean - actuals_clean) / 
                   (np.abs(actuals_clean) + np.abs(predictions_clean)))
    
    # Coverage metrics (simplified)
    coverage = _calculate_prediction_coverage(actuals_clean, predictions_clean)
    
    # Compare against thresholds
    thresholds = eval_config.metrics
    mape_pass = mape <= _safe_config_get(thresholds, 'mape_threshold', 0.15)
    smape_pass = smape <= _safe_config_get(thresholds, 'smape_threshold', 0.15)
    coverage_pass = coverage >= _safe_config_get(thresholds, 'coverage_threshold', 0.8)
    
    metrics = {
        'mape': mape,
        'smape': smape,
        'rmse': rmse,
        'r2_score': r2,
        'coverage': coverage,
        'n_observations': len(actuals_clean),
        'thresholds': {
            'mape_threshold': _safe_config_get(thresholds, 'mape_threshold', 0.15),
            'smape_threshold': _safe_config_get(thresholds, 'smape_threshold', 0.15),
            'coverage_threshold': _safe_config_get(thresholds, 'coverage_threshold', 0.8)
        },
        'passes_thresholds': {
            'mape': mape_pass,
            'smape': smape_pass,
            'coverage': coverage_pass,
            'overall': mape_pass and smape_pass and coverage_pass
        }
    }
    
    logger.info(f"Core metrics: MAPE={mape:.3f}, SMAPE={smape:.3f}, RÂ²={r2:.3f}")
    return metrics


def _calculate_prediction_coverage(actuals: np.ndarray, predictions: np.ndarray) -> float:
    """Calculate prediction interval coverage (simplified)."""
    # Simplified coverage calculation
    residuals = actuals - predictions
    std_residual = np.std(residuals)
    
    # 80% prediction interval (Â±1.28 * std)
    lower_bound = predictions - 1.28 * std_residual
    upper_bound = predictions + 1.28 * std_residual
    
    coverage = np.mean((actuals >= lower_bound) & (actuals <= upper_bound))
    return coverage


def _temporal_holdout_validation(
    features_df: pd.DataFrame,
    target_col: str,
    model_results: Any,
    config: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Perform temporal holdout validation."""
    
    # Get temporal split ratio from config or use default
    training_config = _safe_config_get(config, 'training', {})
    rolling_splits = _safe_config_get(training_config, 'rolling_splits', {})
    window_weeks = _safe_config_get(rolling_splits, 'window_weeks', 104)
    step_weeks = _safe_config_get(rolling_splits, 'step_weeks', 13)
    
    # Calculate split ratio based on rolling window config
    # Default to 80% if config values would create invalid split
    total_weeks = len(features_df)  # Assuming weekly data
    if total_weeks > window_weeks:
        split_ratio = window_weeks / total_weeks
    else:
        split_ratio = 0.8  # Fallback
    
    # Split data temporally
    split_idx = int(len(features_df) * split_ratio)
    
    train_df = features_df.iloc[:split_idx]
    test_df = features_df.iloc[split_idx:]
    
    if len(test_df) < 10:
        logger.warning("Insufficient data for temporal validation")
        return {'error': 'Insufficient holdout data'}
    
    # Generate predictions on holdout
    test_actuals = test_df[target_col].values
    test_predictions = _generate_model_predictions(model_results, test_df, config)
    
    # Calculate holdout metrics
    holdout_metrics = _calculate_core_metrics(
        test_actuals, test_predictions, config.evaluation, logger
    )
    
    # Time series specific checks
    temporal_checks = {
        'trend_consistency': _check_trend_consistency(test_actuals, test_predictions),
        'seasonal_accuracy': _check_seasonal_accuracy(test_df, test_actuals, test_predictions),
        'forecast_drift': _check_forecast_drift(test_actuals, test_predictions)
    }
    
    return {
        'holdout_metrics': holdout_metrics,
        'temporal_checks': temporal_checks,
        'holdout_size': len(test_df),
        'train_size': len(train_df)
    }


def _check_trend_consistency(actuals: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:
    """Check if predictions follow actual trends."""
    # Calculate trend directions
    actual_trend = np.diff(actuals)
    pred_trend = np.diff(predictions)
    
    # Direction agreement
    direction_agreement = np.mean(np.sign(actual_trend) == np.sign(pred_trend))
    
    return {
        'direction_agreement': direction_agreement,
        'trend_correlation': np.corrcoef(actual_trend, pred_trend)[0, 1] if len(actual_trend) > 1 else 0
    }


def _check_seasonal_accuracy(df: pd.DataFrame, actuals: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:
    """Check seasonal pattern accuracy."""
    if 'DATE_DAY' in df.columns:
        dates = pd.to_datetime(df['DATE_DAY'])
        # Month-based seasonality check
        months = dates.dt.month
        
        seasonal_errors = {}
        for month in range(1, 13):
            month_mask = months == month
            if month_mask.sum() > 0:
                month_actuals = actuals[month_mask]
                month_preds = predictions[month_mask]
                if len(month_actuals) > 0:
                    month_mape = mean_absolute_percentage_error(month_actuals, month_preds)
                    seasonal_errors[f'month_{month}'] = month_mape
        
        return {
            'monthly_errors': seasonal_errors,
            'seasonal_consistency': np.std(list(seasonal_errors.values())) if seasonal_errors else 0
        }
    
    return {'error': 'No date column found for seasonal analysis'}


def _check_forecast_drift(actuals: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:
    """Check for systematic forecast drift."""
    errors = predictions - actuals
    
    # Check for systematic bias over time
    cumulative_error = np.cumsum(errors)
    max_drift = np.max(np.abs(cumulative_error))
    
    # Trend in errors
    time_index = np.arange(len(errors))
    error_trend = np.polyfit(time_index, errors, 1)[0] if len(errors) > 1 else 0
    
    return {
        'max_cumulative_drift': max_drift,
        'error_trend': error_trend,
        'drift_severity': 'high' if max_drift > np.std(actuals) else 'low'
    }


def _digital_channel_validation(
    features_df: pd.DataFrame,
    model_results: Any,
    eval_config: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate digital channel performance and attribution."""
    
    digital_checks = eval_config.digital_checks
    validation_results = {}
    
    # Platform attribution comparison
    if _safe_config_get(digital_checks, 'platform_attribution_compare', True):
        validation_results['attribution_comparison'] = _compare_platform_attribution(
            features_df, model_results, logger
        )
    
    # Channel contribution validation
    validation_results['channel_contributions'] = _validate_channel_contributions(
        features_df, model_results, logger
    )
    
    # Incrementality checks
    validation_results['incrementality_checks'] = _validate_incrementality_patterns(
        features_df, model_results, eval_config, logger
    )
    
    return validation_results


def _compare_platform_attribution(
    features_df: pd.DataFrame,
    model_results: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Compare model attribution to platform-reported attribution."""
    
    # This would compare to actual platform data
    # For now, generate synthetic comparison
    
    spend_columns = [col for col in features_df.columns if '_SPEND' in col.upper()]
    attribution_comparison = {}
    
    for col in spend_columns:
        channel = col.replace('_SPEND', '').lower()
        
        # Simulate platform-reported vs model attribution
        platform_attribution = np.random.uniform(0.8, 1.2)  # Â±20% variance
        model_attribution = 1.0  # Baseline
        
        attribution_comparison[channel] = {
            'platform_reported': platform_attribution,
            'model_attributed': model_attribution,
            'difference_pct': (model_attribution - platform_attribution) / platform_attribution,
            'within_tolerance': abs(model_attribution - platform_attribution) / platform_attribution < 0.3
        }
    
    return attribution_comparison


def _validate_channel_contributions(
    features_df: pd.DataFrame,
    model_results: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate channel contribution patterns."""
    
    spend_columns = [col for col in features_df.columns if '_SPEND' in col.upper()]
    
    contribution_metrics = {}
    for col in spend_columns:
        channel = col.replace('_SPEND', '').lower()
        spend_data = features_df[col].fillna(0)
        
        # Calculate contribution metrics
        contribution_metrics[channel] = {
            'spend_share': spend_data.sum() / features_df[spend_columns].sum().sum(),
            'variability': spend_data.std() / spend_data.mean() if spend_data.mean() > 0 else 0,
            'zero_spend_pct': (spend_data == 0).mean(),
            'contribution_stability': _calculate_contribution_stability(spend_data)
        }
    
    return contribution_metrics


def _calculate_contribution_stability(spend_data: pd.Series) -> float:
    """Calculate stability of channel contribution over time."""
    if len(spend_data) < 8:
        return 0.0
    
    # Rolling 4-week contribution share
    rolling_mean = spend_data.rolling(4).mean()
    rolling_std = spend_data.rolling(4).std()
    
    # Coefficient of variation for stability
    cv = rolling_std.mean() / rolling_mean.mean() if rolling_mean.mean() > 0 else float('inf')
    
    # Convert to stability score (lower CV = higher stability)
    stability = 1.0 / (1.0 + cv)
    return stability


def _validate_incrementality_patterns(
    features_df: pd.DataFrame,
    model_results: Any,
    config: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate incrementality patterns in the data."""
    
    spend_columns = [col for col in features_df.columns if '_SPEND' in col.upper()]
    
    incrementality_results = {}
    for col in spend_columns:
        channel = col.replace('_SPEND', '').lower()
        spend_data = features_df[col].fillna(0)
        
        # Simple incrementality check: high spend vs low spend periods
        high_spend_threshold = spend_data.quantile(0.8)
        low_spend_threshold = spend_data.quantile(0.2)
        
        high_spend_mask = spend_data >= high_spend_threshold
        low_spend_mask = spend_data <= low_spend_threshold
        
        if high_spend_mask.sum() > 0 and low_spend_mask.sum() > 0:
            # This would use actual revenue data
            high_spend_performance = 1.2  # Placeholder
            low_spend_performance = 0.8   # Placeholder
            
            # Get incrementality detection threshold from config
            evaluation_config = _safe_config_get(config, 'evaluation', {})
            digital_checks_config = _safe_config_get(evaluation_config, 'digital_checks', {})
            incrementality_threshold = _safe_config_get(digital_checks_config, 'incrementality_threshold', 1.1)
            
            incrementality_results[channel] = {
                'high_spend_performance': high_spend_performance,
                'low_spend_performance': low_spend_performance,
                'incrementality_ratio': high_spend_performance / low_spend_performance,
                'incrementality_detected': high_spend_performance > low_spend_performance * incrementality_threshold
            }
    
    return incrementality_results


def _advanced_feature_validation(
    features_df: pd.DataFrame,
    model_results: Any,
    eval_config: Any,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate advanced feature engineering components."""
    
    advanced_validation = _safe_config_get(eval_config, 'advanced_feature_validation', {})
    validation_results = {}
    
    # Creative fatigue validation
    if 'creative_fatigue' in advanced_validation:
        validation_results['creative_fatigue'] = _validate_creative_fatigue(
            features_df, advanced_validation['creative_fatigue'], logger
        )
    
    # Attribution validation
    if 'attribution' in advanced_validation:
        validation_results['attribution'] = _validate_attribution_features(
            features_df, advanced_validation['attribution'], logger
        )
    
    # Baseline validation
    if 'baseline' in advanced_validation:
        validation_results['baseline'] = _validate_baseline_features(
            features_df, advanced_validation['baseline'], logger
        )
    
    return validation_results


def _validate_creative_fatigue(
    features_df: pd.DataFrame,
    fatigue_config: Dict[str, Any],
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate creative fatigue detection and effects."""
    
    # Look for fatigue-related columns
    fatigue_columns = [col for col in features_df.columns if 'fatigue' in col.lower()]
    
    if not fatigue_columns:
        return {'error': 'No creative fatigue features found'}
    
    results = {}
    for col in fatigue_columns:
        fatigue_data = features_df[col].fillna(1.0)  # Default to no fatigue
        
        # Check fatigue patterns
        refresh_detection_rate = (fatigue_data == 1.0).mean()  # Assuming 1.0 = refresh
        
        # Validate against expected range
        expected_range = fatigue_config.get('refresh_detection_rate', [0.1, 0.3])
        within_range = expected_range[0] <= refresh_detection_rate <= expected_range[1]
        
        results[col] = {
            'refresh_detection_rate': refresh_detection_rate,
            'expected_range': expected_range,
            'within_expected_range': within_range,
            'fatigue_decay_consistency': _check_fatigue_decay_consistency(fatigue_data)
        }
    
    return results


def _check_fatigue_decay_consistency(fatigue_data: pd.Series) -> bool:
    """Check if fatigue decays consistently between refreshes."""
    # Simplified check: fatigue should generally decrease over time
    # until refreshes (increases to 1.0)
    
    refreshes = fatigue_data == 1.0
    if refreshes.sum() < 2:
        return True  # Not enough refreshes to check
    
    # Check decay between refreshes
    refresh_indices = fatigue_data[refreshes].index
    consistent_decay = 0
    total_periods = 0
    
    for i in range(len(refresh_indices) - 1):
        start_idx = refresh_indices[i]
        end_idx = refresh_indices[i + 1]
        
        period_data = fatigue_data.loc[start_idx:end_idx]
        if len(period_data) > 2:
            # Check if generally decreasing
            is_decreasing = period_data.iloc[-1] <= period_data.iloc[0]
            if is_decreasing:
                consistent_decay += 1
            total_periods += 1
    
    return (consistent_decay / total_periods) >= 0.7 if total_periods > 0 else True


def _validate_attribution_features(
    features_df: pd.DataFrame,
    attribution_config: Dict[str, Any],
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate attribution modeling features."""
    
    # Look for attribution-related columns
    attribution_columns = [col for col in features_df.columns if 'attribution' in col.lower()]
    
    if not attribution_columns:
        return {'error': 'No attribution features found'}
    
    results = {}
    
    # Check attribution sum consistency
    total_attribution_sum = [0.95, 1.05]  # Default range
    if 'total_attribution_sum' in attribution_config:
        total_attribution_sum = attribution_config['total_attribution_sum']
    
    for col in attribution_columns:
        attr_data = features_df[col].fillna(0)
        
        # Sum should be close to 1.0 (or proportional)
        mean_attribution = attr_data.mean()
        within_range = total_attribution_sum[0] <= mean_attribution <= total_attribution_sum[1]
        
        results[col] = {
            'mean_attribution': mean_attribution,
            'expected_range': total_attribution_sum,
            'within_expected_range': within_range,
            'attribution_stability': attr_data.std() / attr_data.mean() if attr_data.mean() > 0 else float('inf')
        }
    
    return results


def _validate_baseline_features(
    features_df: pd.DataFrame,
    baseline_config: Dict[str, Any],
    logger: logging.Logger
) -> Dict[str, Any]:
    """Validate baseline feature quality."""
    
    # Look for baseline-related columns
    baseline_columns = [col for col in features_df.columns 
                      if any(term in col.lower() for term in ['trend', 'seasonal', 'baseline', 'holiday'])]
    
    if not baseline_columns:
        return {'error': 'No baseline features found'}
    
    results = {}
    
    for col in baseline_columns:
        baseline_data = features_df[col].fillna(0)
        
        # Check trend smoothness
        if 'trend' in col.lower():
            trend_smoothness = _calculate_trend_smoothness(baseline_data)
            trend_threshold = baseline_config.get('trend_smoothness', 0.1)
            
            results[col] = {
                'trend_smoothness': trend_smoothness,
                'threshold': trend_threshold,
                'passes_smoothness_check': trend_smoothness <= trend_threshold
            }
        
        # General feature validation
        results[col] = results.get(col, {})
        results[col].update({
            'variance': baseline_data.var(),
            'range': baseline_data.max() - baseline_data.min(),
            'missing_pct': baseline_data.isna().mean()
        })
    
    return results


def _calculate_trend_smoothness(trend_data: pd.Series) -> float:
    """Calculate trend smoothness (lower = smoother)."""
    if len(trend_data) < 3:
        return 0.0
    
    # Second derivative (curvature) as smoothness measure
    first_diff = np.diff(trend_data.values)
    second_diff = np.diff(first_diff)
    
    # Normalized smoothness measure
    smoothness = np.std(second_diff) / (np.std(trend_data) + 1e-8)
    return smoothness


def _generate_model_summary(
    model_results: Any,
    features_df: pd.DataFrame,
    actuals: np.ndarray,
    predictions: np.ndarray,
    logger: logging.Logger
) -> Dict[str, Any]:
    """Generate comprehensive model summary."""
    
    return {
        'model_type': 'Hierarchical Bayesian MMM',
        'data_summary': {
            'n_observations': len(features_df),
            'n_features': len(features_df.columns),
            'date_range': {
                'start': features_df['DATE_DAY'].min() if 'DATE_DAY' in features_df.columns else None,
                'end': features_df['DATE_DAY'].max() if 'DATE_DAY' in features_df.columns else None
            },
            'target_summary': {
                'mean': np.mean(actuals),
                'std': np.std(actuals),
                'min': np.min(actuals),
                'max': np.max(actuals)
            }
        },
        'prediction_summary': {
            'mean': np.mean(predictions),
            'std': np.std(predictions),
            'min': np.min(predictions),
            'max': np.max(predictions),
            'prediction_range_ratio': (np.max(predictions) - np.min(predictions)) / (np.max(actuals) - np.min(actuals))
        },
        'convergence_diagnostics': {
            'converged': True,  # Placeholder
            'n_iterations': 1000,  # Placeholder
            'r_hat_max': 1.05  # Placeholder
        }
    }


def _generate_recommendations(
    evaluation_results: Dict[str, Any],
    eval_config: Any,
    logger: logging.Logger
) -> List[str]:
    """Generate actionable recommendations based on evaluation results."""
    
    recommendations = []
    
    # Check core metrics
    if 'metrics' in evaluation_results:
        metrics = evaluation_results['metrics']
        
        if not metrics.get('passes_thresholds', {}).get('overall', False):
            if not metrics.get('passes_thresholds', {}).get('mape', True):
                recommendations.append(f"MAPE ({metrics['mape']:.3f}) exceeds threshold ({metrics['thresholds']['mape_threshold']:.3f}). Consider feature engineering or model complexity adjustments.")
            
            if not metrics.get('passes_thresholds', {}).get('coverage', True):
                recommendations.append(f"Prediction coverage ({metrics['coverage']:.3f}) below threshold ({metrics['thresholds']['coverage_threshold']:.3f}). Consider uncertainty quantification improvements.")
    
    # Check temporal validation
    if 'validation_results' in evaluation_results and 'temporal' in evaluation_results['validation_results']:
        temporal = evaluation_results['validation_results']['temporal']
        if 'temporal_checks' in temporal:
            checks = temporal['temporal_checks']
            if checks.get('trend_consistency', {}).get('direction_agreement', 0) < 0.7:
                recommendations.append("Poor trend direction agreement. Consider adding more trend features or adjusting model structure.")
    
    # Check digital validation
    if 'digital_checks' in evaluation_results:
        digital = evaluation_results['digital_checks']
        if 'attribution_comparison' in digital:
            for channel, attr in digital['attribution_comparison'].items():
                if not attr.get('within_tolerance', True):
                    recommendations.append(f"Attribution for {channel} differs significantly from platform reporting. Validate attribution logic.")
    
    # General recommendations
    if len(recommendations) == 0:
        recommendations.append("Model validation passed all checks. Consider advanced validation techniques or incremental testing.")
    
    return recommendations


def _calculate_overall_score(evaluation_results: Dict[str, Any]) -> float:
    """Calculate overall validation score."""
    
    score_components = []
    
    # Core metrics weight (40%)
    if 'metrics' in evaluation_results:
        metrics = evaluation_results['metrics']
        if metrics.get('passes_thresholds', {}).get('overall', False):
            score_components.append(0.4)
        else:
            # Partial credit based on individual metrics
            individual_scores = [
                0.15 if metrics.get('passes_thresholds', {}).get('mape', False) else 0,
                0.15 if metrics.get('passes_thresholds', {}).get('smape', False) else 0,
                0.1 if metrics.get('passes_thresholds', {}).get('coverage', False) else 0
            ]
            score_components.append(sum(individual_scores))
    
    # Temporal validation weight (30%)
    if 'validation_results' in evaluation_results and 'temporal' in evaluation_results['validation_results']:
        temporal = evaluation_results['validation_results']['temporal']
        if 'temporal_checks' in temporal:
            checks = temporal['temporal_checks']
            trend_score = checks.get('trend_consistency', {}).get('direction_agreement', 0)
            score_components.append(0.3 * trend_score)
    
    # Digital checks weight (20%)
    if 'digital_checks' in evaluation_results:
        digital = evaluation_results['digital_checks']
        if 'attribution_comparison' in digital:
            attr_scores = [attr.get('within_tolerance', False) 
                          for attr in digital['attribution_comparison'].values()]
            if attr_scores:
                score_components.append(0.2 * (sum(attr_scores) / len(attr_scores)))
    
    # Feature validation weight (10%)
    if 'feature_validation' in evaluation_results:
        score_components.append(0.1)  # Simplified scoring
    
    overall_score = sum(score_components)
    return min(1.0, overall_score)  # Cap at 1.0


def save_evaluation_results(
    results: Dict[str, Any],
    output_path: Optional[str] = None,
    config: Optional[Any] = None
) -> str:
    """
    Save evaluation results to disk.
    
    Args:
        results: Evaluation results to save
        output_path: Optional custom output path
        config: Configuration object
        
    Returns:
        str: Path where results were saved
    """
    if config is None:
        config = load_config()
    
    if output_path is None:
        output_dir = Path(config.paths.artifacts) / "evaluation"
        output_dir.mkdir(parents=True, exist_ok=True)
        run_id = results.get('run_id', 'unknown')
        output_path = output_dir / f"evaluation_results_{run_id}.json"
    
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Save results as JSON
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    return str(output_path)


def run_model_evaluation():
    """
    CLI entry point for model evaluation.
    
    Loads model results and features, runs evaluation, saves results.
    """
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        # Load configuration
        config = load_config()
        
        # Load model results - detect backend and use appropriate file
        models_path = Path(config.paths.models)
        backend = config.model.backend
        
        if backend == "pymc":
            model_files = list(models_path.glob("pymc_fit_result.pkl"))
        elif backend == "meridian":
            model_files = list(models_path.glob("meridian_fit_result.pkl"))
        else:
            model_files = list(models_path.glob("*_fit_result.pkl"))
        
        if not model_files:
            logger.error(f"No model results found for backend '{backend}'. Please run model training first: mmm train")
            return False
        
        logger.info(f"Loading model results from: {model_files[0]}")
        with open(model_files[0], 'rb') as f:
            model_results = pickle.load(f)
        
        # Load features data
        features_path = Path(config.paths.features) / "engineered_features.parquet"
        if not features_path.exists():
            logger.error("No features data found. Please run feature engineering first: mmm features")
            return False
        
        logger.info(f"Loading features from: {features_path}")
        features_df = pd.read_parquet(features_path)
        
        # Run evaluation
        results = evaluate_model(
            model_results=model_results,
            features_df=features_df,
            config=config
        )
        
        # Save results
        output_path = save_evaluation_results(results, config=config)
        
        # Log summary
        logger.info("ðŸŽ‰ Model evaluation completed successfully!")
        logger.info(f"Results saved to: {output_path}")
        logger.info(f"Overall validation score: {results['overall_score']:.3f}")
        
        # Log key metrics
        if 'metrics' in results:
            metrics = results['metrics']
            logger.info(f"Core metrics: MAPE={metrics['mape']:.3f}, RÂ²={metrics['r2_score']:.3f}")
        
        # Log recommendations
        if 'recommendations' in results:
            logger.info("Key recommendations:")
            for i, rec in enumerate(results['recommendations'][:3], 1):  # Top 3
                logger.info(f"  {i}. {rec}")
        
        return True
        
    except Exception as e:
        logger.error(f"Model evaluation failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    run_model_evaluation()
